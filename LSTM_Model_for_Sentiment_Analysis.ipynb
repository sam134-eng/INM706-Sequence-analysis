{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam134-eng/INM706-Sequence-analysis/blob/main/LSTM_Model_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Download NLTK data unconditionally at the start\n",
        "# This ensures 'punkt' and 'stopwords' are available for text processing\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# --- 1. Load and Preprocess Data ---\n",
        "# Assuming 'Tweets.csv' is available in the environment.\n",
        "# In a Colab environment, you would typically upload it first.\n",
        "# For this self-contained code, we'll assume it's loaded.\n",
        "# If running locally, ensure 'Tweets.csv' is in the same directory.\n",
        "try:\n",
        "    df = pd.read_csv(\"Tweets.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Tweets.csv' not found. Please ensure the file is uploaded or in the correct directory.\")\n",
        "    # Create a dummy DataFrame for demonstration if file is not found\n",
        "    data = {\n",
        "        'text': [\n",
        "            \"@VirginAmerica What @dhepburn said.\",\n",
        "            \"@VirginAmerica plus you've added commercials to the experience... #fail\",\n",
        "            \"@VirginAmerica I didn't today... Must mean I need to fly it again!\",\n",
        "            \"@VirginAmerica it's really aggressive to blast obnoxious \"\n",
        "            \"loud commercials into little earbuds. #anditshardtohearanyone\",\n",
        "            \"@VirginAmerica and it's a really big bad thing about it\"\n",
        "        ],\n",
        "        'airline_sentiment': ['neutral', 'negative', 'positive', 'negative', 'negative']\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Using a dummy DataFrame for demonstration.\")\n",
        "\n",
        "\n",
        "# Keep only relevant columns: 'text' and 'airline_sentiment'\n",
        "df = df[['text', 'airline_sentiment']]\n",
        "\n",
        "# Filter to ensure only 'positive', 'neutral', 'negative' sentiments are included\n",
        "df = df[df['airline_sentiment'].isin(['positive', 'neutral', 'negative'])]\n",
        "\n",
        "# Map sentiment labels to integers for model training\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['label'] = df['airline_sentiment'].map(label_map)\n",
        "\n",
        "# Define stopwords for text cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by:\n",
        "    - Removing URLs, mentions (@), hashtags (#)\n",
        "    - Removing non-alphabetic characters\n",
        "    - Converting to lowercase\n",
        "    - Tokenizing the text\n",
        "    - Removing stopwords\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"http\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]\", \"\", text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([t for t in tokens if t not in stop_words]) # Join tokens back to string for TF-IDF/Tokenizer\n",
        "\n",
        "# Apply the cleaning function to the 'text' column\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# --- 2. Prepare Data for LSTM ---\n",
        "# Define features (X) and labels (y)\n",
        "X = df['clean_text']\n",
        "y = df['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Keras Tokenizer\n",
        "# num_words: the maximum number of words to keep, based on word frequency.\n",
        "# oov_token: token to represent out-of-vocabulary words.\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train) # Fit tokenizer only on training data\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to ensure uniform input length for the LSTM layer\n",
        "# maxlen: maximum length of all sequences. Sequences longer than this are truncated,\n",
        "# and sequences shorter are padded.\n",
        "maxlen = 50 # Example max length, can be tuned\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "# --- 3. Load Pre-trained Word Embeddings (GloVe) ---\n",
        "# This function assumes 'glove.6B.100d.txt' is available.\n",
        "# You would typically download this file (e.g., from Stanford's GloVe page)\n",
        "# and place it in your working directory.\n",
        "def load_glove(tokenizer, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    Loads pre-trained GloVe word embeddings and creates an embedding matrix\n",
        "    for the words in the tokenizer's vocabulary.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Keras Tokenizer object, fitted on the text data.\n",
        "        embedding_dim (int): Dimension of the GloVe embeddings (e.g., 50, 100, 200, 300).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Embedding matrix where each row corresponds to a word\n",
        "                       in the tokenizer's vocabulary and its pre-trained embedding vector.\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    glove_file_path = f\"glove.6B.{embedding_dim}d.txt\"\n",
        "    try:\n",
        "        with open(glove_file_path, encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = vector\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: '{glove_file_path}' not found. Please download GloVe embeddings \"\n",
        "              \"and place the file in the correct directory.\")\n",
        "        print(\"Proceeding with random embeddings for demonstration purposes.\")\n",
        "        # Create a dummy embeddings_index if GloVe file is not found\n",
        "        for word, i in tokenizer.word_index.items():\n",
        "            embeddings_index[word] = np.random.rand(embedding_dim)\n",
        "\n",
        "\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        vec = embeddings_index.get(word)\n",
        "        if vec is not None:\n",
        "            embedding_matrix[i] = vec\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load GloVe embeddings (assuming 100d version is used)\n",
        "embedding_dim = 100\n",
        "embedding_matrix = load_glove(tokenizer, embedding_dim)\n",
        "\n",
        "# --- 4. Build the LSTM Model ---\n",
        "model = Sequential()\n",
        "# Embedding layer:\n",
        "# input_dim: size of the vocabulary (number of unique words + 1 for padding/OOV).\n",
        "# output_dim: dimension of the dense embedding.\n",
        "# weights: pre-trained embedding matrix.\n",
        "# input_length: length of input sequences (maxlen).\n",
        "# trainable: False to keep embeddings fixed, True to fine-tune them during training.\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxlen,\n",
        "                    trainable=False)) # Set to True if you want to fine-tune GloVe\n",
        "\n",
        "# LSTM layer:\n",
        "# 128 units: number of LSTM units (neurons).\n",
        "# return_sequences=False: returns only the last output sequence, suitable for classification.\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.5)) # Dropout for regularization to prevent overfitting\n",
        "\n",
        "# Dense layers for classification:\n",
        "# 64 units: hidden layer with ReLU activation.\n",
        "# 3 units: output layer for 3 sentiment classes (negative, neutral, positive) with softmax activation.\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax')) # 3 classes: negative (0), neutral (1), positive (2)\n",
        "\n",
        "# Compile the model\n",
        "# loss: 'sparse_categorical_crossentropy' is used when labels are integers (0, 1, 2).\n",
        "# optimizer: 'adam' is a popular and effective optimizer.\n",
        "# metrics: 'accuracy' to monitor classification accuracy during training.\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# --- 5. Train the Model (Optional - uncomment to run training) ---\n",
        "# Note: Training can be time-consuming and requires a 'Tweets.csv' file and 'glove.6B.100d.txt'.\n",
        "# history = model.fit(X_train_pad, y_train_np,\n",
        "#                     epochs=10, # Number of training epochs\n",
        "#                     batch_size=32, # Number of samples per gradient update\n",
        "#                     validation_data=(X_test_pad, y_test_np)) # Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
        "\n",
        "# --- 6. Evaluate the Model (Optional - uncomment to run evaluation) ---\n",
        "# loss, accuracy = model.evaluate(X_test_pad, y_test_np)\n",
        "# print(f\"Test Loss: {loss:.4f}\")\n",
        "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# --- Example Prediction (Optional) ---\n",
        "# new_texts = [\n",
        "#     \"This airline is amazing, I love their service!\",\n",
        "#     \"My flight was delayed and the customer service was terrible.\",\n",
        "#     \"The flight was okay, nothing special.\"\n",
        "# ]\n",
        "# new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "# new_padded_sequences = pad_sequences(new_sequences, maxlen=maxlen)\n",
        "# predictions = model.predict(new_padded_sequences)\n",
        "# predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# sentiment_labels = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "# for i, text in enumerate(new_texts):\n",
        "#     print(f\"Text: '{text}' -> Predicted Sentiment: {sentiment_labels[predicted_classes[i]]}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'Tweets.csv' not found. Please ensure the file is uploaded or in the correct directory.\n",
            "Using a dummy DataFrame for demonstration.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9dde190ccce1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Apply the cleaning function to the 'text' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# --- 2. Prepare Data for LSTM ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9dde190ccce1>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n\u001b[1;32m     66\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Join tokens back to string for TF-IDF/Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "SxclAeliIEvA",
        "outputId": "c85de9ea-4870-42a1-d832-152bf08bfc6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Download NLTK data unconditionally at the start\n",
        "# This ensures 'punkt', 'stopwords', and 'punkt_tab' are available for text processing\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# Download punkt_tab resource explicitly as it seems to be required by word_tokenize in this context\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# --- Download and extract GloVe embeddings ---\n",
        "# This is necessary to load pre-trained word vectors later in the script.\n",
        "# The file glove.6B.100d.txt is expected by the load_glove function.\n",
        "\n",
        "# Download the GloVe 6B dataset (contains embeddings of different dimensions)\n",
        "# This command downloads the zip file from the Stanford NLP website.\n",
        "# It's a large file, so it may take some time depending on your internet connection.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "# This extracts all files within the zip, including glove.6B.100d.txt,\n",
        "# into your current working directory.\n",
        "!unzip -o glove.6B.zip # -o option overwrites existing files without prompting\n",
        "\n",
        "\n",
        "# --- 1. Load and Preprocess Data ---\n",
        "# Assuming 'Tweets.csv' is available in the environment.\n",
        "# In a Colab environment, you would typically upload it first.\n",
        "# For this self-contained code, we'll assume it's loaded.\n",
        "# If running locally, ensure 'Tweets.csv' is in the same directory.\n",
        "try:\n",
        "    df = pd.read_csv(\"Tweets.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Tweets.csv' not found. Please ensure the file is uploaded or in the correct directory.\")\n",
        "    # Create a dummy DataFrame for demonstration if file is not found\n",
        "    # Modified dummy data to have at least two examples per class for stratified split\n",
        "    data = {\n",
        "        'text': [\n",
        "            \"@VirginAmerica What @dhepburn said.\", # Neutral\n",
        "            \"@VirginAmerica plus you've added commercials to the experience... #fail\", # Negative\n",
        "            \"@VirginAmerica I didn't today... Must mean I need to fly it again!\", # Positive\n",
        "            \"@VirginAmerica it's really aggressive to blast obnoxious \"\n",
        "            \"loud commercials into little earbuds. #anditshardtohearanyone\", # Negative\n",
        "            \"@VirginAmerica and it's a really big bad thing about it\", # Negative\n",
        "            \"This is a great flight, excellent service!\", # Positive\n",
        "            \"Everything was just okay, nothing to write home about.\", # Neutral\n",
        "            \"My bags were lost, terrible experience.\", # Negative\n",
        "            \"Loved the onboard entertainment.\", # Positive\n",
        "            \"The food was decent.\", # Neutral\n",
        "            \"Worst flight of my life, never flying again.\" # Negative\n",
        "        ],\n",
        "        'airline_sentiment': ['neutral', 'negative', 'positive', 'negative', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'negative']\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Using a dummy DataFrame with more data for demonstration.\")\n",
        "\n",
        "\n",
        "# Keep only relevant columns: 'text' and 'airline_sentiment'\n",
        "df = df[['text', 'airline_sentiment']]\n",
        "\n",
        "# Filter to ensure only 'positive', 'neutral', 'negative' sentiments are included\n",
        "df = df[df['airline_sentiment'].isin(['positive', 'neutral', 'negative'])]\n",
        "\n",
        "# Map sentiment labels to integers for model training\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['label'] = df['airline_sentiment'].map(label_map)\n",
        "\n",
        "# Define stopwords for text cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by:\n",
        "    - Removing URLs, mentions (@), hashtags (#)\n",
        "    - Removing non-alphabetic characters\n",
        "    - Converting to lowercase\n",
        "    - Tokenizing the text\n",
        "    - Removing stopwords\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"http\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]\", \"\", text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([t for t in tokens if t not in stop_words]) # Join tokens back to string for TF-IDF/Tokenizer\n",
        "\n",
        "# Apply the cleaning function to the 'text' column\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# --- 2. Prepare Data for LSTM ---\n",
        "# Define features (X) and labels (y)\n",
        "X = df['clean_text']\n",
        "y = df['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Keras Tokenizer\n",
        "# num_words: the maximum number of words to keep, based on word frequency.\n",
        "# oov_token: token to represent out-of-vocabulary words.\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train) # Fit tokenizer only on training data\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to ensure uniform input length for the LSTM layer\n",
        "# maxlen: maximum length of all sequences. Sequences longer than this are truncated,\n",
        "# and sequences shorter are padded.\n",
        "maxlen = 50 # Example max length, can be tuned\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "# --- 3. Load Pre-trained Word Embeddings (GloVe) ---\n",
        "# This function assumes 'glove.6B.100d.txt' is available.\n",
        "# You would typically download this file (e.g., from Stanford's GloVe page)\n",
        "# and place it in your working directory.\n",
        "def load_glove(tokenizer, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    Loads pre-trained GloVe word embeddings and creates an embedding matrix\n",
        "    for the words in the tokenizer's vocabulary.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Keras Tokenizer object, fitted on the text data.\n",
        "        embedding_dim (int): Dimension of the GloVe embeddings (e.g., 50, 100, 200, 300).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Embedding matrix where each row corresponds to a word\n",
        "                       in the tokenizer's vocabulary and its pre-trained embedding vector.\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    glove_file_path = f\"glove.6B.{embedding_dim}d.txt\"\n",
        "    try:\n",
        "        with open(glove_file_path, encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = vector\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {glove_file_path} not found.\")\n",
        "        print(\"Please download GloVe embeddings (e.g., glove.6B.zip from Stanford)\")\n",
        "        print(\"and extract the appropriate file into the working directory.\")\n",
        "        return None # Return None if file is not found\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    num_words = min(tokenizer.num_words, len(word_index) + 1) # Number of words in tokenizer vocab\n",
        "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    print(f\"Found {len(embeddings_index)} word vectors in GloVe.\")\n",
        "    print(f\"Created embedding matrix of shape: {embedding_matrix.shape}\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load the GloVe embedding matrix\n",
        "embedding_dim = 100 # Match the dimension of the GloVe file you intend to use\n",
        "embedding_matrix = load_glove(tokenizer, embedding_dim)\n",
        "\n",
        "# --- 4. Build the LSTM Model ---\n",
        "# Only build the model if embedding_matrix was successfully loaded\n",
        "if embedding_matrix is not None:\n",
        "    vocab_size = min(tokenizer.num_words, len(tokenizer.word_index) + 1) # Vocabulary size\n",
        "    num_classes = len(label_map) # Number of sentiment classes (3: negative, neutral, positive)\n",
        "\n",
        "    model = Sequential([\n",
        "        # Embedding layer using pre-trained GloVe weights\n",
        "        # input_dim: vocabulary size\n",
        "        # output_dim: dimension of the dense embedding (same as embedding_dim)\n",
        "        # input_length: length of input sequences (maxlen)\n",
        "        # weights: pre-trained weights from GloVe\n",
        "        # trainable: set to False to keep embeddings fixed, True to fine-tune them\n",
        "        Embedding(input_dim=vocab_size,\n",
        "                  output_dim=embedding_dim,\n",
        "                  input_length=maxlen,\n",
        "                  weights=[embedding_matrix],\n",
        "                  trainable=False), # Set to True to fine-tune embeddings\n",
        "        LSTM(units=128, return_sequences=False), # LSTM layer, return_sequences=False for classification\n",
        "        Dropout(0.5), # Dropout for regularization\n",
        "        Dense(units=64, activation='relu'), # Dense layer\n",
        "        Dropout(0.5), # Dropout for regularization\n",
        "        Dense(units=num_classes, activation='softmax') # Output layer with softmax for multi-class classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    # optimizer: e.g., 'adam', 'rmsprop'\n",
        "    # loss: 'sparse_categorical_crossentropy' for integer labels\n",
        "    # metrics: 'accuracy' to monitor performance\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # --- 5. Train the Model ---\n",
        "    epochs = 5 # Number of training epochs, can be tuned\n",
        "    batch_size = 32 # Batch size, can be tuned\n",
        "\n",
        "    # Check if training data is sufficient\n",
        "    if len(X_train_pad) > 0:\n",
        "        history = model.fit(X_train_pad, y_train_np,\n",
        "                            epochs=epochs,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.2) # Use 20% of training data for validation\n",
        "    else:\n",
        "        print(\"Not enough training data to fit the model.\")\n",
        "else:\n",
        "    print(\"Model training skipped because GloVe embedding matrix was not loaded.\")\n",
        "\n",
        "# --- 6. Evaluate the Model (if trained) ---\n",
        "if 'model' in locals() and len(X_test_pad) > 0:\n",
        "    print(\"\\nEvaluating the model on the test set...\")\n",
        "    loss, accuracy = model.evaluate(X_test_pad, y_test_np, verbose=0)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "elif 'model' not in locals():\n",
        "     print(\"\\nModel was not built, skipping evaluation.\")\n",
        "else:\n",
        "    print(\"\\nNot enough test data to evaluate the model.\")\n",
        "\n",
        "\n",
        "# --- 7. Make Predictions (Optional) ---\n",
        "if 'model' in locals():\n",
        "    print(\"\\nMaking predictions on the test set...\")\n",
        "    predictions = model.predict(X_test_pad)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Map integer predictions back to sentiment labels\n",
        "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "    predicted_sentiments = [reverse_label_map[pred] for pred in predicted_classes]\n",
        "\n",
        "    # Display some sample predictions\n",
        "    print(\"Sample Predictions:\")\n",
        "    for i in range(min(10, len(X_test))): # Display first 10 predictions\n",
        "        original_text = df.iloc[X_test.index[i]]['text'] # Get original text from the original df\n",
        "        actual_sentiment = df.iloc[X_test.index[i]]['airline_sentiment'] # Get actual sentiment\n",
        "        print(f\"  Text: {original_text}\")\n",
        "        print(f\"  Actual Sentiment: {actual_sentiment}\")\n",
        "        print(f\"  Predicted Sentiment: {predicted_sentiments[i]}\")\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"\\nSkipping prediction as the model was not built or trained.\")\n",
        "\n",
        "# --- Example of predicting on a new sentence ---\n",
        "if 'model' in locals() and 'tokenizer' in locals() and maxlen is not None:\n",
        "    def predict_sentiment(text, model, tokenizer, maxlen, label_map):\n",
        "        \"\"\"\n",
        "        Predicts the sentiment of a single new text input.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text string.\n",
        "            model: The trained Keras model.\n",
        "            tokenizer: The fitted Keras Tokenizer.\n",
        "            maxlen (int): The maximum sequence length used during training.\n",
        "            label_map (dict): Dictionary mapping sentiment strings to integers.\n",
        "\n",
        "        Returns:\n",
        "            str: The predicted sentiment label ('negative', 'neutral', 'positive').\n",
        "        \"\"\"\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(text)\n",
        "        # Convert to sequence\n",
        "        seq = tokenizer.texts_to_sequences([cleaned_text])\n",
        "        # Pad the sequence\n",
        "        padded_seq = pad_sequences(seq, maxlen=maxlen)\n",
        "        # Make prediction\n",
        "        prediction = model.predict(padded_seq)[0]\n",
        "        # Get the predicted class index\n",
        "        predicted_class_index = np.argmax(prediction)\n",
        "        # Map index back to label\n",
        "        reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "        predicted_sentiment = reverse_label_map[predicted_class_index]\n",
        "        return predicted_sentiment\n",
        "\n",
        "    # Example usage\n",
        "    new_sentence1 = \"The flight was delayed, but the staff was helpful.\"\n",
        "    predicted_sentiment1 = predict_sentiment(new_sentence1, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"\\nSentiment for '{new_sentence1}': {predicted_sentiment1}\")\n",
        "\n",
        "    new_sentence2 = \"Absolutely loved the movie!\"\n",
        "    predicted_sentiment2 = predict_sentiment(new_sentence2, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"Sentiment for '{new_sentence2}': {predicted_sentiment2}\")\n",
        "\n",
        "    new_sentence3 = \"This is just an average experience.\"\n",
        "    predicted_sentiment3 = predict_sentiment(new_sentence3, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"Sentiment for '{new_sentence3}': {predicted_sentiment3}\")\n",
        "\n",
        "    new_sentence4 = \"Terrible customer service.\"\n",
        "    predicted_sentiment4 = predict_sentiment(new_sentence4, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"Sentiment for '{new_sentence4}': {predicted_sentiment4}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping prediction on new sentences as model or tokenizer is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tZapiweInWB",
        "outputId": "014c8513-6f0c-4b00-e0c0-65bb652e6fed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 12:40:39--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-23 12:40:39--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-23 12:40:40--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2025-05-23 12:43:19 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Found 400000 word vectors in GloVe.\n",
            "Created embedding matrix of shape: (5000, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m500,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m500,000\u001b[0m (1.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> (1.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m500,000\u001b[0m (1.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> (1.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 91ms/step - accuracy: 0.6749 - loss: 0.7917 - val_accuracy: 0.7392 - val_loss: 0.6399\n",
            "Epoch 2/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 80ms/step - accuracy: 0.7441 - loss: 0.6411 - val_accuracy: 0.7636 - val_loss: 0.5896\n",
            "Epoch 3/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 77ms/step - accuracy: 0.7573 - loss: 0.5893 - val_accuracy: 0.7665 - val_loss: 0.5744\n",
            "Epoch 4/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.7815 - loss: 0.5524 - val_accuracy: 0.7678 - val_loss: 0.5788\n",
            "Epoch 5/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7761 - loss: 0.5496 - val_accuracy: 0.7789 - val_loss: 0.5518\n",
            "\n",
            "Evaluating the model on the test set...\n",
            "Test Loss: 0.5651\n",
            "Test Accuracy: 0.7705\n",
            "\n",
            "Making predictions on the test set...\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
            "Sample Predictions:\n",
            "  Text: @united past\n",
            "  Actual Sentiment: neutral\n",
            "  Predicted Sentiment: neutral\n",
            "--------------------\n",
            "  Text: @JetBlue would you say a delay is more likely? Thanks so much.\n",
            "  Actual Sentiment: positive\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @JetBlue I cheated on you, and I'm sorry. I'll never do it again. @SouthwestAir has given my wife and I the worst start to a honeymoon ever\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @united disappointed that u didnt honor my $100 credit given to me for ur mistakes. Taking my business elsewhere  ✌️out.\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @USAirways The airline is embarrassing itself. I get that bad weather isn't your fault, but your response to it couldn't have been worse.\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @AmericanAir my flight has been delayed &amp; I can't talk reach anyone in customer service because of high call volume.\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @united This link in your tweet goes to someone's internal email -&gt; http://t.co/ZksX79itdN...... Probably one of your 3rd party IT contracts\n",
            "  Actual Sentiment: neutral\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @SouthwestAir BETSY is the BESTY! Gettin' stuck at #LAS might not be bad for most..but I want home! #homewardbound #betsy #besty #thankyou\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @USAirways How many agents do you have working to handle the thousands of calls?\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @SouthwestAir When will the flight resume? I don's see it in the open schedule. :/\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: neutral\n",
            "--------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\n",
            "Sentiment for 'The flight was delayed, but the staff was helpful.': negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Sentiment for 'Absolutely loved the movie!': positive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Sentiment for 'This is just an average experience.': negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Sentiment for 'Terrible customer service.': negative\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}