{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam134-eng/INM706-Sequence-analysis/blob/main/LSTM_Model_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Download NLTK data unconditionally at the start\n",
        "# This ensures 'punkt', 'stopwords', and 'punkt_tab' are available for text processing\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# Download punkt_tab resource explicitly as it seems to be required by word_tokenize in this context\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# --- Download and extract GloVe embeddings ---\n",
        "# This is necessary to load pre-trained word vectors later in the script.\n",
        "# The file glove.6B.100d.txt is expected by the load_glove function.\n",
        "\n",
        "# Download the GloVe 6B dataset (contains embeddings of different dimensions)\n",
        "# This command downloads the zip file from the Stanford NLP website.\n",
        "# It's a large file, so it may take some time depending on your internet connection.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "# This extracts all files within the zip, including glove.6B.100d.txt,\n",
        "# into your current working directory.\n",
        "!unzip -o glove.6B.zip # -o option overwrites existing files without prompting\n",
        "\n",
        "\n",
        "# --- 1. Load and Preprocess Data ---\n",
        "# Assuming 'Tweets.csv' is available in the environment.\n",
        "# In a Colab environment, you would typically upload it first.\n",
        "# For this self-contained code, we'll assume it's loaded.\n",
        "# If running locally, ensure 'Tweets.csv' is in the same directory.\n",
        "try:\n",
        "    df = pd.read_csv(\"Tweets.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Tweets.csv' not found. Please ensure the file is uploaded or in the correct directory.\")\n",
        "    # Create a dummy DataFrame for demonstration if file is not found\n",
        "    # Modified dummy data to have at least two examples per class for stratified split\n",
        "    data = {\n",
        "        'text': [\n",
        "            \"@VirginAmerica What @dhepburn said.\", # Neutral\n",
        "            \"@VirginAmerica plus you've added commercials to the experience... #fail\", # Negative\n",
        "            \"@VirginAmerica I didn't today... Must mean I need to fly it again!\", # Positive\n",
        "            \"@VirginAmerica it's really aggressive to blast obnoxious \"\n",
        "            \"loud commercials into little earbuds. #anditshardtohearanyone\", # Negative\n",
        "            \"@VirginAmerica and it's a really big bad thing about it\", # Negative\n",
        "            \"This is a great flight, excellent service!\", # Positive\n",
        "            \"Everything was just okay, nothing to write home about.\", # Neutral\n",
        "            \"My bags were lost, terrible experience.\", # Negative\n",
        "            \"Loved the onboard entertainment.\", # Positive\n",
        "            \"The food was decent.\", # Neutral\n",
        "            \"Worst flight of my life, never flying again.\" # Negative\n",
        "        ],\n",
        "        'airline_sentiment': ['neutral', 'negative', 'positive', 'negative', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'negative']\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Using a dummy DataFrame with more data for demonstration.\")\n",
        "\n",
        "\n",
        "# Keep only relevant columns: 'text' and 'airline_sentiment'\n",
        "df = df[['text', 'airline_sentiment']]\n",
        "\n",
        "# Filter to ensure only 'positive', 'neutral', 'negative' sentiments are included\n",
        "df = df[df['airline_sentiment'].isin(['positive', 'neutral', 'negative'])]\n",
        "\n",
        "# Map sentiment labels to integers for model training\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['label'] = df['airline_sentiment'].map(label_map)\n",
        "\n",
        "# Define stopwords for text cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by:\n",
        "    - Removing URLs, mentions (@), hashtags (#)\n",
        "    - Removing non-alphabetic characters\n",
        "    - Converting to lowercase\n",
        "    - Tokenizing the text\n",
        "    - Removing stopwords\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"http\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]\", \"\", text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([t for t in tokens if t not in stop_words]) # Join tokens back to string for TF-IDF/Tokenizer\n",
        "\n",
        "# Apply the cleaning function to the 'text' column\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# --- 2. Prepare Data for LSTM ---\n",
        "# Define features (X) and labels (y)\n",
        "X = df['clean_text']\n",
        "y = df['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Keras Tokenizer\n",
        "# num_words: the maximum number of words to keep, based on word frequency.\n",
        "# oov_token: token to represent out-of-vocabulary words.\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train) # Fit tokenizer only on training data\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to ensure uniform input length for the LSTM layer\n",
        "# maxlen: maximum length of all sequences. Sequences longer than this are truncated,\n",
        "# and sequences shorter are padded.\n",
        "maxlen = 50 # Example max length, can be tuned\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "# --- 3. Load Pre-trained Word Embeddings (GloVe) ---\n",
        "# This function assumes 'glove.6B.100d.txt' is available.\n",
        "# You would typically download this file (e.g., from Stanford's GloVe page)\n",
        "# and place it in your working directory.\n",
        "def load_glove(tokenizer, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    Loads pre-trained GloVe word embeddings and creates an embedding matrix\n",
        "    for the words in the tokenizer's vocabulary.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Keras Tokenizer object, fitted on the text data.\n",
        "        embedding_dim (int): Dimension of the GloVe embeddings (e.g., 50, 100, 200, 300).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Embedding matrix where each row corresponds to a word\n",
        "                       in the tokenizer's vocabulary and its pre-trained embedding vector.\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    glove_file_path = f\"glove.6B.{embedding_dim}d.txt\"\n",
        "    try:\n",
        "        with open(glove_file_path, encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = vector\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {glove_file_path} not found.\")\n",
        "        print(\"Please download GloVe embeddings (e.g., glove.6B.zip from Stanford)\")\n",
        "        print(\"and extract the appropriate file into the working directory.\")\n",
        "        return None # Return None if file is not found\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    num_words = min(tokenizer.num_words, len(word_index) + 1) # Number of words in tokenizer vocab\n",
        "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    print(f\"Found {len(embeddings_index)} word vectors in GloVe.\")\n",
        "    print(f\"Created embedding matrix of shape: {embedding_matrix.shape}\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load the GloVe embedding matrix\n",
        "embedding_dim = 100 # Match the dimension of the GloVe file you intend to use\n",
        "embedding_matrix = load_glove(tokenizer, embedding_dim)\n",
        "\n",
        "# --- 4. Build the LSTM Model ---\n",
        "# Only build the model if embedding_matrix was successfully loaded\n",
        "if embedding_matrix is not None:\n",
        "    vocab_size = min(tokenizer.num_words, len(tokenizer.word_index) + 1) # Vocabulary size\n",
        "    num_classes = len(label_map) # Number of sentiment classes (3: negative, neutral, positive)\n",
        "\n",
        "    model = Sequential([\n",
        "        # Embedding layer using pre-trained GloVe weights\n",
        "        # input_dim: vocabulary size\n",
        "        # output_dim: dimension of the dense embedding (same as embedding_dim)\n",
        "        # input_length: length of input sequences (maxlen)\n",
        "        # weights: pre-trained weights from GloVe\n",
        "        # trainable: set to False to keep embeddings fixed, True to fine-tune them\n",
        "        Embedding(input_dim=vocab_size,\n",
        "                  output_dim=embedding_dim,\n",
        "                  input_length=maxlen,\n",
        "                  weights=[embedding_matrix],\n",
        "                  trainable=False), # Set to True to fine-tune embeddings\n",
        "        LSTM(units=128, return_sequences=False), # LSTM layer, return_sequences=False for classification\n",
        "        Dropout(0.5), # Dropout for regularization\n",
        "        Dense(units=64, activation='relu'), # Dense layer\n",
        "        Dropout(0.5), # Dropout for regularization\n",
        "        Dense(units=num_classes, activation='softmax') # Output layer with softmax for multi-class classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    # optimizer: e.g., 'adam', 'rmsprop'\n",
        "    # loss: 'sparse_categorical_crossentropy' for integer labels\n",
        "    # metrics: 'accuracy' to monitor performance\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # --- 5. Train the Model ---\n",
        "    epochs = 5 # Number of training epochs, can be tuned\n",
        "    batch_size = 32 # Batch size, can be tuned\n",
        "\n",
        "    # Check if training data is sufficient\n",
        "    if len(X_train_pad) > 0:\n",
        "        history = model.fit(X_train_pad, y_train_np,\n",
        "                            epochs=epochs,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.2) # Use 20% of training data for validation\n",
        "    else:\n",
        "        print(\"Not enough training data to fit the model.\")\n",
        "else:\n",
        "    print(\"Model training skipped because GloVe embedding matrix was not loaded.\")\n",
        "\n",
        "# --- 6. Evaluate the Model (if trained) ---\n",
        "if 'model' in locals() and len(X_test_pad) > 0:\n",
        "    print(\"\\nEvaluating the model on the test set...\")\n",
        "    loss, accuracy = model.evaluate(X_test_pad, y_test_np, verbose=0)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "elif 'model' not in locals():\n",
        "     print(\"\\nModel was not built, skipping evaluation.\")\n",
        "else:\n",
        "    print(\"\\nNot enough test data to evaluate the model.\")\n",
        "\n",
        "\n",
        "# --- 7. Make Predictions (Optional) ---\n",
        "if 'model' in locals():\n",
        "    print(\"\\nMaking predictions on the test set...\")\n",
        "    predictions = model.predict(X_test_pad)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Map integer predictions back to sentiment labels\n",
        "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "    predicted_sentiments = [reverse_label_map[pred] for pred in predicted_classes]\n",
        "\n",
        "    # Display some sample predictions\n",
        "    print(\"Sample Predictions:\")\n",
        "    for i in range(min(10, len(X_test))): # Display first 10 predictions\n",
        "        original_text = df.iloc[X_test.index[i]]['text'] # Get original text from the original df\n",
        "        actual_sentiment = df.iloc[X_test.index[i]]['airline_sentiment'] # Get actual sentiment\n",
        "        print(f\"  Text: {original_text}\")\n",
        "        print(f\"  Actual Sentiment: {actual_sentiment}\")\n",
        "        print(f\"  Predicted Sentiment: {predicted_sentiments[i]}\")\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"\\nSkipping prediction as the model was not built or trained.\")\n",
        "\n",
        "# --- Example of predicting on a new sentence ---\n",
        "if 'model' in locals() and 'tokenizer' in locals() and maxlen is not None:\n",
        "    def predict_sentiment(text, model, tokenizer, maxlen, label_map):\n",
        "        \"\"\"\n",
        "        Predicts the sentiment of a single new text input.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text string.\n",
        "            model: The trained Keras model.\n",
        "            tokenizer: The fitted Keras Tokenizer.\n",
        "            maxlen (int): The maximum sequence length used during training.\n",
        "            label_map (dict): Dictionary mapping sentiment strings to integers.\n",
        "\n",
        "        Returns:\n",
        "            str: The predicted sentiment label ('negative', 'neutral', 'positive').\n",
        "        \"\"\"\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(text)\n",
        "        # Convert to sequence\n",
        "        seq = tokenizer.texts_to_sequences([cleaned_text])\n",
        "        # Pad the sequence\n",
        "        padded_seq = pad_sequences(seq, maxlen=maxlen)\n",
        "        # Make prediction\n",
        "        prediction = model.predict(padded_seq)[0]\n",
        "        # Get the predicted class index\n",
        "        predicted_class_index = np.argmax(prediction)\n",
        "        # Map index back to label\n",
        "        reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "        predicted_sentiment = reverse_label_map[predicted_class_index]\n",
        "        return predicted_sentiment\n",
        "\n",
        "    # Example usage\n",
        "    new_sentence1 = \"The flight was delayed, but the staff was helpful.\"\n",
        "    predicted_sentiment1 = predict_sentiment(new_sentence1, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"\\nSentiment for '{new_sentence1}': {predicted_sentiment1}\")\n",
        "\n",
        "    new_sentence2 = \"Absolutely loved the movie!\"\n",
        "    predicted_sentiment2 = predict_sentiment(new_sentence2, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"Sentiment for '{new_sentence2}': {predicted_sentiment2}\")\n",
        "\n",
        "    new_sentence3 = \"This is just an average experience.\"\n",
        "    predicted_sentiment3 = predict_sentiment(new_sentence3, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"Sentiment for '{new_sentence3}': {predicted_sentiment3}\")\n",
        "\n",
        "    new_sentence4 = \"Terrible customer service.\"\n",
        "    predicted_sentiment4 = predict_sentiment(new_sentence4, model, tokenizer, maxlen, label_map)\n",
        "    print(f\"Sentiment for '{new_sentence4}': {predicted_sentiment4}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping prediction on new sentences as model or tokenizer is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tZapiweInWB",
        "outputId": "014c8513-6f0c-4b00-e0c0-65bb652e6fed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 12:40:39--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-23 12:40:39--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-23 12:40:40--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2025-05-23 12:43:19 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Found 400000 word vectors in GloVe.\n",
            "Created embedding matrix of shape: (5000, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m500,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m500,000\u001b[0m (1.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> (1.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m500,000\u001b[0m (1.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> (1.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 91ms/step - accuracy: 0.6749 - loss: 0.7917 - val_accuracy: 0.7392 - val_loss: 0.6399\n",
            "Epoch 2/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 80ms/step - accuracy: 0.7441 - loss: 0.6411 - val_accuracy: 0.7636 - val_loss: 0.5896\n",
            "Epoch 3/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 77ms/step - accuracy: 0.7573 - loss: 0.5893 - val_accuracy: 0.7665 - val_loss: 0.5744\n",
            "Epoch 4/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.7815 - loss: 0.5524 - val_accuracy: 0.7678 - val_loss: 0.5788\n",
            "Epoch 5/5\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7761 - loss: 0.5496 - val_accuracy: 0.7789 - val_loss: 0.5518\n",
            "\n",
            "Evaluating the model on the test set...\n",
            "Test Loss: 0.5651\n",
            "Test Accuracy: 0.7705\n",
            "\n",
            "Making predictions on the test set...\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
            "Sample Predictions:\n",
            "  Text: @united past\n",
            "  Actual Sentiment: neutral\n",
            "  Predicted Sentiment: neutral\n",
            "--------------------\n",
            "  Text: @JetBlue would you say a delay is more likely? Thanks so much.\n",
            "  Actual Sentiment: positive\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @JetBlue I cheated on you, and I'm sorry. I'll never do it again. @SouthwestAir has given my wife and I the worst start to a honeymoon ever\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @united disappointed that u didnt honor my $100 credit given to me for ur mistakes. Taking my business elsewhere  ✌️out.\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @USAirways The airline is embarrassing itself. I get that bad weather isn't your fault, but your response to it couldn't have been worse.\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @AmericanAir my flight has been delayed &amp; I can't talk reach anyone in customer service because of high call volume.\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @united This link in your tweet goes to someone's internal email -&gt; http://t.co/ZksX79itdN...... Probably one of your 3rd party IT contracts\n",
            "  Actual Sentiment: neutral\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @SouthwestAir BETSY is the BESTY! Gettin' stuck at #LAS might not be bad for most..but I want home! #homewardbound #betsy #besty #thankyou\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @USAirways How many agents do you have working to handle the thousands of calls?\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: negative\n",
            "--------------------\n",
            "  Text: @SouthwestAir When will the flight resume? I don's see it in the open schedule. :/\n",
            "  Actual Sentiment: negative\n",
            "  Predicted Sentiment: neutral\n",
            "--------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\n",
            "Sentiment for 'The flight was delayed, but the staff was helpful.': negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Sentiment for 'Absolutely loved the movie!': positive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Sentiment for 'This is just an average experience.': negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Sentiment for 'Terrible customer service.': negative\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}